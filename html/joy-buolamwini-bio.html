<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Joy Buolamwini, 'poet of code'</title>
    <link rel="stylesheet" href="../css/base.css">
    <link rel="stylesheet" href="../css/articles.css">
</head>
<body>
    <header>
        <a href="../index.html"><h1>A journey in computational thinking...</h1></a>
        </header>
    <nav>
        <ul>
            <li><a href="what-is-computational-thinking.html">| | What IS computational thinking? | |</a></li>
            <li class = "current">| | A computer scientist I admire | |</li>
            <li><a href="reflections.html">| | Learning reflections | |</a></li>
        </ul>
    </nav>

    <main>
        <article>
            <h2>Joy Buolamwini, founder of the Algorithmic Justice League</h2>
            <figure>
                <img src="../images/joy_wef.jpg" alt="Joy Buolamwini speaking at an event">
                <!-- Image free to use with attribution under CC BY-NC-SA 2.0 -->
                <figcaption class="img-caption"><a href="https://www.flickr.com/photos/worldeconomicforum/45937998865/in/photostream/">"Compassion through Computation: Fighting Algorithmic Bias"</a> by <a href="https://www.flickr.com/photos/worldeconomicforum/">World Economic Forum</a>, licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">CC BY-NC-SA 2.0</a></figcaption>
            </figure>
            <p>Joy Buolamwini is a computer scientist and digital activist who has pioneered academic work on algorithmic bias with a far-reaching impact (Eynon 2020).</p>
            <p>Based at the MIT Media Lab, Buolamwini advocates for equitable and accountable AI through research, poetry and her non-profit organisation, the <a href="https://www.ajl.org/">Algorithmic Justice League.</a></p>
            <p>Her work revealing gender and racial biases in facial recognition systems led to mainstream criticism of these algorithms (Peters 2020) and has been credited with eventually driving large tech companies to drop facial recognition services (Metcalf et al. 2022).</p>
            <p>She is also the main subject of Netflix documentary <cite>Coded Bias</cite> (2020), which follows her journey to push for the US to govern against bias in algorithms.</p>
            
            <h3>Early life</h3>
            <p>Buolamwini was born in Canada to an artist mother and a pharmaceutical sciences professor father (Beauvais 2018).</p>
            <p>She spent her early childhood in Ghana and moved to Mississippi in the United States at the age of four (Carnegie Corporation of New York 2020).</p>
            <p>The Ghanaian-American was drawn to computer science from a young age. In high school, she taught herself XHTML, CSS, JavaScript, PHP and web design by reading online tutorials (Beauvais 2018).</p>

            <h3>Toward algorithmic justice</h3>
            <p>While studying computer science as an undergraduate at Georgia Tech and later on as a graduate student at the MIT Media Lab, Buolamwini worked on several projects that used facial recognition technology.</p>
            <p>When testing her projects, she repeatedly ran into the same problem: the technology could not detect her face – that of a darker skinned woman. However, if she wore a white mask or got a lighter skinned person to test the software for her, it worked (TED 2017).</p>
            <figure>
                <blockquote cite="https://www.youtube.com/watch?v=UG_X_7g63rY&ab_channel=TED" class="quote">“Algorithmic bias, like human bias, results in unfairness. However, algorithms, like viruses, can spread bias on a massive scale at a rapid pace.”</blockquote>
                <figcaption class="quote attribution">– Joy Buolamwini (TED, 2017)</figcaption>
            </figure>
            <p>Inspired by these experiences, Buolamwini subsequently co-authored a study with Timnit Gebru (Buolamwini and Gebru, 2018) analysing the commercial gender classification systems of facial recognition systems by Microsoft, IBM and Face++. </p>
            <p>The study found that the datasets powering facial analysis algorithms were “overwhelmingly composed of lighter skinned subjects”, especially men. As a result of this biased input, lighter skinned men were classified almost perfectly by the systems tested, with error rates of at most 0.8 percent, while misclassifications of darker skinned women were as high as 34.5 percent.</p>

            <h3>Widespread influence</h3>
            <p>Buolamwini and Gebru’s 2018 study, <cite>Gender Shades</cite>, set off a movement.</p>
            <p>Within six months of its publication, Microsoft and IBM released new versions of their facial analysis APIs with improved accuracy, major companies like Google established fairness organisations and US senators called on the FBI to review the accuracy of its automated facial analysis tools (Gebru 2020, p. 258).</p>
            <p>As D’Ignazio and Klein (2020, p. 32) note, Buolamwini draws attention to instances of algorithmic biases not only through academic research but also in the “realm of culture and ideas” by producing viral poetry projects and giving TED talks, as well as advising on legislation and professional standards for the field of computer vision through the Algorithmic Justice League.</p>
            <div class="embed-youtube">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/QxuyfWoVV98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
            <p>Testifying in Congress in 2019, Buolamwini called for the US government to consider adopting a moratorium on law enforcement use of face recognition or other facial analysis technologies unless regulations were adopted (Congress.gov 2019).</p>
            <p>Her initiatives “present a compelling model for the data scientist as public intellectual – who, yes, works on technical audits and fixes, but also works on cultural, legal, and political efforts too”, D'Ignazio and Klein (2020) write in the book <cite>Data Feminism</cite>.</p>
            
            <hr>

            <h5>References</h5>
            <ol type="1">
                <li>Beauvais, E. 2018. The future of computer science and tech: 12 young women to watch – Part 2. <cite>Amy Poehler's Smart Girls</cite>. Available at: <a href="https://amysmartgirls.com/the-future-of-computer-science-and-tech-12-young-women-to-watch-part-2-334c2282025d">https://amysmartgirls.com/the-future-of-computer-science-and-tech-12-young-women-to-watch-part-2-334c2282025d</a> [Accessed 28 October 2022]</li>
                <li>Buolamwini, J. and Gebru, T. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. <cite>Proceedings of Machine Learning Research 81</cite>, pp. 77-91.</li>
                <li>Carnegie Corporation of New York. 2020. <cite>Joy Buolamwini</cite>. Available at: <a href="https://www.carnegie.org/awards/honoree/joy-buolamwini/">https://www.carnegie.org/awards/honoree/joy-buolamwini/</a> [Accessed 30 October 2022]</li>
                <li><cite>Coded Bias</cite>. 2020. Directed by Shalini Kantayya. [Documentary]. Available at: Netflix. [Accessed 15 October 2022]</li>
                <li>Congress.gov. 2019. Written testimony of Joy Buolamwini. Available at: <a href="https://www.congress.gov/116/meeting/house/109521/witnesses/HHRG-116-GO00-Wstate-BuolamwiniJ-20190522.pdf">https://www.congress.gov/116/meeting/house/109521/witnesses/HHRG-116-GO00-Wstate-BuolamwiniJ-20190522.pdf</a> [Accessed 30 October 2022]</li>
                <li>D’Ignazio, C. and Klein, L.F. 2020. <cite>Data Feminism</cite>. Cambridge, Massachusetts: The MIT Press.</li>
                <li>Eynon, R. 2020. <cite>Black heroes of the internet – Joy Buolamwini</cite>. Available at: <a href="https://www.oii.ox.ac.uk/news-events/news/black-heroes-of-the-internet-joy-buolamwini/">https://www.oii.ox.ac.uk/news-events/news/black-heroes-of-the-internet-joy-buolamwini/</a> [Accessed 29 October 2022]</li>
                <li>Gebru, T. 2020. Race and Gender. In: Markus, D.D., Pasquale, F. and Das, S. eds. <cite>The Oxford Handbook of Ethics of AI</cite>. New York: Oxford University Press, pp. 252–269.</li>
                <li>Metcalf, J., Smith, B. and Moss, E. 2022. A new proposed law could actually hold big tech accountable for its algorithms. <cite>Slate</cite> 9 February. Available at: <a href="https://slate.com/technology/2022/02/algorithmic-accountability-act-wyden.html">https://slate.com/technology/2022/02/algorithmic-accountability-act-wyden.html</a> [Accessed 30 October 2022]</li>
                <li>Peters, J. 2020. IBM will no longer offer, develop, or research facial recognition technology. <cite>The Verge</cite> 9 June. Available at: <a href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software">https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software</a> [Accessed 30 October 2022]</li>
                <li>TED. 2017. <cite>How I'm fighting bias in algorithms | Joy Buolamwini</cite>. Available at: <a href="https://www.youtube.com/watch?v=UG_X_7g63rY">https://www.youtube.com/watch?v=UG_X_7g63rY</a> [Accessed 25 October 2022]</li>
                <li>Wood, M. 2019. Thoughts on recent research paper and associated article on Amazon Rekognition. <cite>AWS Machine Learning Blog</cite>. Available at: <a href="https://aws.amazon.com/blogs/machine-learning/thoughts-on-recent-research-paper-and-associated-article-on-amazon-rekognition/">https://aws.amazon.com/blogs/machine-learning/thoughts-on-recent-research-paper-and-associated-article-on-amazon-rekognition/</a> [Accessed 30 October 2022]</li>
            </ol>
        </article>
    </main>

    <footer>&copy; Melissa Zhu, CMT119 Semester 1, 2022/2023</footer>
</body>
</html>